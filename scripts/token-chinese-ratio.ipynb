{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = json.load(open('poet.tang.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model('text-davinci-003')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model('gpt-3.5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "enc = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56315\n",
      "{'id': '3ad6d468-7ff1-4a7b-8b24-a27d70d00ed4', 'title': '帝京篇十首 一', 'author': '太宗皇帝', 'content': '秦川雄帝宅，函谷壮皇居。\\n绮殿千寻起，离宫百雉余。\\n连甍遥接汉，飞观迥凌虚。\\n云日隐层阙，风烟出绮疎。'}\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "print(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 244/244 [00:00<00:00, 307kB/s]\n",
      "tokenization_chatglm.py: 100%|██████████| 10.1k/10.1k [00:00<00:00, 9.83MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/THUDM/chatglm2-6b:\n",
      "- tokenization_chatglm.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "tokenizer.model: 100%|██████████| 1.02M/1.02M [00:00<00:00, 40.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "enc = AutoTokenizer.from_pretrained(\"THUDM/chatglm2-6b\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 802/802 [00:00<00:00, 965kB/s]\n",
      "tokenization_baichuan.py: 100%|██████████| 9.58k/9.58k [00:00<00:00, 8.40MB/s]\n",
      "A new version of the following files was downloaded from https://huggingface.co/baichuan-inc/Baichuan-7B:\n",
      "- tokenization_baichuan.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "tokenizer.model: 100%|██████████| 1.14M/1.14M [00:00<00:00, 340MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 411/411 [00:00<00:00, 528kB/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers.generation.utils import GenerationConfig\n",
    "enc = AutoTokenizer.from_pretrained(\"baichuan-inc/Baichuan-7B\",\n",
    "    #revision=\"v2.0\",\n",
    "    use_fast=False,\n",
    "    trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 174/174 [00:00<00:00, 116kB/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/Qwen/Qwen-7B-Chat:\n",
      "- tokenization_qwen.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "enc = AutoTokenizer.from_pretrained(\"Qwen/Qwen-7B-Chat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ',', 'Ġthis', 'Ġis', 'Ġa', 'Ġtest', 'Ġinput', '.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"claude-v1-tokenization.json\")\n",
    "text = \"Hello, this is a test input.\"\n",
    "tokens = fast_tokenizer.tokenize(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = fast_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(poem):\n",
    "    # ids = enc.encode_ordinary(example['text']) # encode_ordinary ignores any special tokens\n",
    "    # ids.append(enc.eot_token) # add the end of text token, e.g. 50256 for gpt2 bpe\n",
    "    # out = {'ids': ids, 'len': len(ids)}\n",
    "    text = poem['title'] + '\\n' + poem['author'] + '\\n' + poem['content']\n",
    "    ids = enc.encode(text)\n",
    "    # ids.append(enc.eot_token)\n",
    "    return text, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_ids = map(process, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4116019 5799479\n",
      "1.4090019992619083\n"
     ]
    }
   ],
   "source": [
    "len_text = 0\n",
    "len_tokens = 0\n",
    "\n",
    "for t_id in t_ids:\n",
    "    # print(len(t_id[0]))\n",
    "    # print(len(t_id[1]))\n",
    "    len_text += len(t_id[0])\n",
    "    len_tokens += len(t_id[1])\n",
    "\n",
    "print(len_text, len_tokens)\n",
    "print(len_tokens/len_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
